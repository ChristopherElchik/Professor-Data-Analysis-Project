{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Aquisition\n",
    "This cell scrapes the most up-to-date data on all NCSU professors on RateMyProfessors and saves the data to a JSON file. This takes less than 30 seconds to execute, and does not need to be re-run if the file already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ratemyprofessors import RateMyProfessorsAPI\n",
    "\n",
    "\n",
    "api = RateMyProfessorsAPI()\n",
    "school = api.search_school(\"NCSU\")\n",
    "\n",
    "professors = []\n",
    "\n",
    "# Get all NCSU professors on RateMyProfessors\n",
    "cursor = \"\"\n",
    "while True:\n",
    "    result = api.search_teachers(school['id'], \"\", limit=1000, cursor=cursor)\n",
    "    if not result['teachers']:\n",
    "        break\n",
    "    cursor = result['end_cursor']\n",
    "    for professor in result['teachers']:\n",
    "        del professor['school']\n",
    "    professors.extend(result['teachers'])\n",
    "\n",
    "print(len(professors), 'professors fetched')\n",
    "\n",
    "# Filter out professors with no ratings\n",
    "professors = list(filter(lambda professor: professor['num_ratings'] > 0, professors))\n",
    "\n",
    "print(len(professors), 'professors with ratings')\n",
    "\n",
    "# Sort by most ratings in descending order    \n",
    "professors.sort(key=lambda professor: professor['num_ratings'], reverse=True)\n",
    "\n",
    "with open(\"data/professors.json\", \"w\") as file:\n",
    "    json.dump(professors, file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell scrapes distributions for every course from the gradient database, calculating the average GPA for each section. It is designed to be able to be computed in multiple executions in case the authorization tokens expire before the scraping is complete. This process takes multiple hours to complete.\n",
    "\n",
    "To execute this cell, you must have valid authentication headers in a file named `gradient-headers.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from gradient import GradientAPI\n",
    "\n",
    "\n",
    "with open(\"data/colleges.json\") as file:\n",
    "    colleges = json.load(file)\n",
    "\n",
    "try:\n",
    "    with open(\"data/distributions.json\", \"r\") as file:\n",
    "        distributions = json.load(file)\n",
    "        last_subject, last_course = distributions[-1][\"courseName\"].split()[:2]\n",
    "except:\n",
    "    distributions = []\n",
    "    last_subject = last_course = None\n",
    "\n",
    "\n",
    "api = GradientAPI(request_delay=2)\n",
    "\n",
    "found = False\n",
    "try:\n",
    "    for college in colleges:\n",
    "        for subject in colleges[college][\"subjects\"]:\n",
    "            # Check if we have already fetched this subject\n",
    "            if last_subject and not found:\n",
    "                if subject != last_subject:\n",
    "                    continue\n",
    "                found = True\n",
    "            \n",
    "            for course_distributions in api.get_subject_distrubutions(subject, last_course if subject == last_subject else None):\n",
    "                if \"individual\" not in course_distributions:\n",
    "                    continue\n",
    "                \n",
    "                for section in course_distributions[\"individual\"]:\n",
    "                    a, b, c, d, f, s, u, w = (section[\"grades\"][grade][\"raw\"] for grade in [\"A\", \"B\", \"C\", \"D\", \"F\", \"S\", \"U\", \"W\"])\n",
    "                    total = a + b + c + d + f + s + u + w\n",
    "                    gpa = round((a * 4 + b * 3 + c * 2 + d + s * 3) / total, 2) if total else 0\n",
    "                    \n",
    "                    section[\"gpa\"] = gpa\n",
    "                    section[\"total\"] = total\n",
    "                    section[\"college\"] = college\n",
    "                    del section[\"grades\"]\n",
    "                    del section[\"googleChart\"]\n",
    "                \n",
    "                distributions.extend(course_distributions[\"individual\"])\n",
    "except:\n",
    "    # gracefully handle exceptions or interruptions\n",
    "    pass\n",
    "finally:\n",
    "    with open(\"data/distributions.json\", \"w\") as file:\n",
    "        json.dump(distributions, file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preprocessing\n",
    "\n",
    "This cell processes the data and tags each professor with their college using a file generated by an LLM and manually validated that maps college names to department names. It then combines duplicate professors (same name and same department) into a single entry with a weighted average of their ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import difflib\n",
    "\n",
    "\n",
    "with open(\"data/professors.json\", \"r\") as file:\n",
    "    professors = json.load(file)\n",
    "    \n",
    "with open(\"data/colleges.json\", \"r\") as file:\n",
    "    colleges = json.load(file)\n",
    "\n",
    "\n",
    "# Construct a map from department to college\n",
    "department_map = {department: college for college, departments in colleges.items() for department in departments}\n",
    "\n",
    "# Fuzzy search for the best department match and assign the college\n",
    "for professor in professors:\n",
    "    department = professor['department']\n",
    "    result = difflib.get_close_matches(department, department_map.keys(), cutoff=0.75)\n",
    "    professor['college'] = department_map[result[0]] if result else None\n",
    "\n",
    "\n",
    "# Combine duplicate entries of the same professor that belong to the same college\n",
    "names = set()\n",
    "duplicates_names = []\n",
    "\n",
    "for professor in professors:\n",
    "    name = professor['name']\n",
    "    if name in names and name not in duplicates_names:\n",
    "        duplicates_names.append(name)\n",
    "    names.add(name)\n",
    "\n",
    "duplicates = {}\n",
    "for professor in filter(lambda professor: professor['name'] in duplicates_names, professors):\n",
    "    key = (professor['name'], professor['college'])\n",
    "    if key not in duplicates:\n",
    "        duplicates[key] = professor\n",
    "    else:\n",
    "        # Take weighted average of two entries\n",
    "        n1, n2 = duplicates[key]['num_ratings'], professor['num_ratings']\n",
    "        total = n1 + n2\n",
    "        avg1, avg2 = duplicates[key]['avg_rating'], professor['avg_rating']\n",
    "        take1, take2 = duplicates[key]['would_take_again'], professor['would_take_again']\n",
    "        diff1, diff2 = duplicates[key]['avg_difficulty'], professor['avg_difficulty']\n",
    "        \n",
    "        duplicates[key]['num_ratings'] = total\n",
    "        duplicates[key]['avg_rating'] = round((avg1 * n1 + avg2 * n2) / total, 1)\n",
    "        duplicates[key]['would_take_again'] = round((take1 * n1 + take2 * n2) / total, 1)\n",
    "        duplicates[key]['avg_difficulty'] = round((diff1 * n1 + diff2 * n2) / total, 1)\n",
    "    \n",
    "\n",
    "professors = list(filter(lambda professor: professor['name'] not in duplicates_names, professors)) + list(duplicates.values())\n",
    "\n",
    "\n",
    "with open(\"data/professors.json\", \"w\") as file:\n",
    "    json.dump(professors, file, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell processes and aggregates the gradient data for all professors that exist on ratemyprofessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import difflib\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"data/professors.json\", \"r\") as file:\n",
    "    professors = {professor[\"name\"].lower(): professor for professor in json.load(file)}  # {name: {professor info}}\n",
    "    professor_names = [professor[\"name\"].lower() for professor in professors.values()]\n",
    "    \n",
    "with open(\"data/distributions.json\", \"r\") as file:\n",
    "    distributions = json.load(file)\n",
    "\n",
    "# To be filled in manually and then converted to dataframe (and then stored as .csv)\n",
    "professors_info = {\"Name\": [], \"College\": [], \"Quality Score\": [], \"Difficulty Score\": [], \"GPA\": [], \"Would Take Again\": [], \"Number of Ratings\": [], \"Number of Sections\": [], \"Total Students\": []}\n",
    "\n",
    "name_map = {}\n",
    "not_found = []\n",
    "missed_names = set()\n",
    "collisions = 0\n",
    "for section in distributions:\n",
    "    full_name = \" \".join(section[\"instructorName\"].split(\",\", 1)[::-1])\n",
    "\n",
    "    # drop middle name and roman numeral and phd when searching for name\n",
    "    split_name = full_name.split()\n",
    "    if all(c in 'IV' for c in split_name[-1].upper()) or split_name[-1].lower() == 'phd':\n",
    "        name = ' '.join([split_name[0], split_name[-2]])\n",
    "    else:\n",
    "        name = ' '.join([split_name[0], split_name[-1]])\n",
    "    result = difflib.get_close_matches(name.lower(), professor_names, n=1, cutoff=0.87)\n",
    "    if not result:\n",
    "        not_found.append(section)\n",
    "        missed_names.add(full_name)\n",
    "        continue\n",
    "    result = result[0]\n",
    "\n",
    "    # fill in professor's info in df\n",
    "    prof_info = professors[result]\n",
    "    try:\n",
    "        # case: professor already added to table, just accumulate stats\n",
    "        idx = professors_info[\"Name\"].index(prof_info['name'])\n",
    "        professors_info[\"GPA\"][idx] += section['gpa'] * section['total']\n",
    "        professors_info[\"Number of Sections\"][idx] += 1\n",
    "        professors_info[\"Total Students\"][idx] += section['total']\n",
    "    except:\n",
    "        # case: professor not in table yet, initialize everything\n",
    "        professors_info[\"Name\"].append(prof_info['name'])\n",
    "        professors_info[\"College\"].append(prof_info['college'])\n",
    "        professors_info[\"Quality Score\"].append(prof_info['avg_rating'])\n",
    "        professors_info[\"Difficulty Score\"].append(prof_info['avg_difficulty'])\n",
    "        professors_info[\"GPA\"].append(section['gpa'] * section['total'])\n",
    "        professors_info[\"Would Take Again\"].append(prof_info['would_take_again'])\n",
    "        professors_info[\"Number of Ratings\"].append(prof_info['num_ratings'])\n",
    "        professors_info[\"Number of Sections\"].append(1)\n",
    "        professors_info[\"Total Students\"].append(section['total'])\n",
    "\n",
    "# Average out GPA\n",
    "for i in range(len(professors_info[\"GPA\"])):\n",
    "    professors_info[\"GPA\"][i] /= professors_info[\"Total Students\"][i]\n",
    "\n",
    "# convert to df and store as csv and json\n",
    "df = pd.DataFrame(professors_info)\n",
    "df.to_json(\"data/combined_data.json\", compression='infer')\n",
    "df.to_csv(\"data/combined_data.csv\", compression='infer')\n",
    "\n",
    "# save items not found\n",
    "with open(\"data/sections_not_found.json\", \"w\") as file:\n",
    "    json.dump(not_found, file, indent=2)\n",
    "\n",
    "# save unique names that weren't found\n",
    "with open(\"data/missed_names.txt\", \"w\") as file:\n",
    "    file.write(str(missed_names))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
